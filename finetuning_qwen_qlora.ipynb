{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Complete QLoRA Fine-tuning Script for Qwen2.5-7B\n",
        "Uses 4-bit quantization with PEFT/LoRA adapters\n",
        "Optimized for Google Colab with single GPU\n",
        ""
      ],
      "metadata": {
        "id": "FfYbAeYHoYBv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3SBqy16oVTW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers datasets peft bitsandbytes accelerate trl\n",
        "!pip install -q -U sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M_ZQjRiJozrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B\"\n",
        "OUTPUT_DIR = \"./qwen2.5-7b-qlora-finetuned\"\n",
        "ADAPTER_DIR = \"./qwen2.5-7b-qlora-adapter\"\n",
        "\n",
        "# QLoRA configuration (as specified)\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "# Training hyperparameters\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 3\n",
        "WARMUP_STEPS = 100\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 100\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_NAME = \"Open-Orca/OpenOrca\"  # Can switch to OpenOrca-Slim or OpenOrca-Platypus2\n",
        "MAX_SAMPLES = 5000  # Limit samples for faster training on Colab\n",
        "\n",
        "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "AvxjtoiCozoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This reduces memory usage from ~28GB to ~7GB for Qwen2.5-7B\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enable 4-bit loading\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use NormalFloat4 quantization\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
        "    bnb_4bit_use_double_quant=True,  # Nested quantization for more memory savings\n",
        ")\n",
        "\n",
        "print(\"✓ 4-bit quantization config created\")"
      ],
      "metadata": {
        "id": "v1ipspCuozk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loading model: {MODEL_NAME}...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"  # Important for decoder-only models\n",
        ")\n",
        "\n",
        "# Set pad token if not exists (required for batching)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load model in 4-bit with quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n"
      ],
      "metadata": {
        "id": "kwhlJZo0ozg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable caching for training help saves memory\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1  # Tensor parallelism setting\n",
        "\n",
        "print(\"✓ Model and tokenizer loaded successfully\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Model dtype: {model.dtype}\")"
      ],
      "metadata": {
        "id": "TVPp979HozeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model for k-bit training (gradient checkpointing, input requires_grad, etc.)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA with specified parameters\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,  # Rank of the low-rank matrices\n",
        "    lora_alpha=LORA_ALPHA,  # Scaling factor\n",
        "    target_modules=TARGET_MODULES,  # Which layers to apply LoRA to\n",
        "    lora_dropout=LORA_DROPOUT,  # Dropout probability\n",
        "    bias=\"none\",  # Don't train bias parameters\n",
        "    task_type=\"CAUSAL_LM\",  # Task type for language modeling\n",
        ")"
      ],
      "metadata": {
        "id": "KM7h5VjTozZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = 0\n",
        "all_params = 0\n",
        "for _, param in model.named_parameters():\n",
        "    all_params += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"✓ LoRA adapters applied\")\n",
        "print(f\"Trainable params: {trainable_params:,} || All params: {all_params:,} || Trainable%: {100 * trainable_params / all_params:.2f}%\")\n"
      ],
      "metadata": {
        "id": "joEhCIBZpapf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLoading dataset: {DATASET_NAME}...\")\n",
        "\n",
        "# Load OpenOrca dataset from Hugging Face\n",
        "# Options: \"Open-Orca/OpenOrca\", \"Open-Orca/OpenOrca-Slim\", or custom\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\", streaming=False)\n",
        "\n",
        "# Take a subset for faster training on Colab\n",
        "dataset = dataset.select(range(min(MAX_SAMPLES, len(dataset))))\n",
        "\n",
        "print(f\"✓ Dataset loaded: {len(dataset)} samples\")\n",
        "print(f\"Dataset columns: {dataset.column_names}\")"
      ],
      "metadata": {
        "id": "Ft71c0vJpbBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_template(example):\n",
        "    \"\"\"\n",
        "    Format dataset examples using the specified chat template:\n",
        "    <|system|>SYSTEM_MESSAGE</|system|>\n",
        "    <|user|>USER_MESSAGE</|user|>\n",
        "    <|assistant|>ASSISTANT_RESPONSE</|assistant|>\n",
        "    \"\"\"\n",
        "    # OpenOrca format: 'system_prompt', 'question', 'response'\n",
        "    system_msg = example.get('system_prompt', 'You are a helpful assistant.')\n",
        "    user_msg = example.get('question', '')\n",
        "    assistant_msg = example.get('response', '')\n",
        "\n",
        "    # Create formatted text\n",
        "    formatted_text = (\n",
        "        f\"<|system|>{system_msg}</|system|>\\n\"\n",
        "        f\"<|user|>{user_msg}</|user|>\\n\"\n",
        "        f\"<|assistant|>{assistant_msg}</|assistant|>\"\n",
        "    )\n",
        "\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# Apply formatting to dataset\n",
        "print(\"Formatting dataset with chat template...\")\n",
        "formatted_dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    remove_columns=dataset.column_names,  # Remove original columns\n",
        "    desc=\"Formatting dataset\"\n",
        ")"
      ],
      "metadata": {
        "id": "-qpM8ctkpbF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and eval sets (90/10 split)\n",
        "split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"✓ Dataset formatted\")\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Eval samples: {len(eval_dataset)}\")\n",
        "print(f\"\\nExample formatted text:\\n{train_dataset[0]['text'][:300]}...\\n\")"
      ],
      "metadata": {
        "id": "oM3s5ZnepmEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "\n",
        "    # Optimizer settings\n",
        "    optim=\"paged_adamw_8bit\",  # 8-bit optimizer for memory efficiency\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # Precision settings\n",
        "    bf16=True,  # Use bfloat16 mixed precision\n",
        "    fp16=False,\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Evaluation\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "\n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    # Other settings\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "print(\"✓ Training arguments configured\")"
      ],
      "metadata": {
        "id": "9TFvUeFepmB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SFTTrainer (Supervised Fine-Tuning Trainer) from TRL\n",
        "# This handles the text field automatically\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",  # Field containing the formatted text\n",
        "    max_seq_length=MAX_LENGTH,\n",
        "    packing=False,  # Don't pack multiple samples together\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer initialized\")"
      ],
      "metadata": {
        "id": "hDkJCpgcpl-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "wBTLjON1pl7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Saving LoRA adapter to {ADAPTER_DIR}...\")\n",
        "\n",
        "# Save only the LoRA adapter weights (much smaller than full model)\n",
        "model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "\n",
        "print(\"✓ LoRA adapter saved successfully\")"
      ],
      "metadata": {
        "id": "rbSU0r2Kply-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATING MODEL\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "riyBQqMsplvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"INFERENCE EXAMPLE\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Prepare model for inference\n",
        "model.eval()\n",
        "\n",
        "def generate_response(system_prompt, user_prompt, max_new_tokens=256):\n",
        "    \"\"\"Generate a response using the fine-tuned model\"\"\"\n",
        "\n",
        "    # Format input using chat template\n",
        "    input_text = (\n",
        "        f\"<|system|>{system_prompt}</|system|>\\n\"\n",
        "        f\"<|user|>{user_prompt}</|user|>\\n\"\n",
        "        f\"<|assistant|>\"\n",
        "    )\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode and extract assistant response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # Extract only the assistant's response\n",
        "    if \"<|assistant|>\" in full_response:\n",
        "        assistant_response = full_response.split(\"<|assistant|>\")[-1]\n",
        "        assistant_response = assistant_response.replace(\"</s>\", \"\").strip()\n",
        "    else:\n",
        "        assistant_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return assistant_response"
      ],
      "metadata": {
        "id": "FqJosc8WqIzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    {\n",
        "        \"system\": \"You are a helpful AI assistant.\",\n",
        "        \"user\": \"What is machine learning?\"\n",
        "    },\n",
        "    {\n",
        "        \"system\": \"You are a coding expert.\",\n",
        "        \"user\": \"Write a Python function to calculate factorial.\"\n",
        "    },\n",
        "    {\n",
        "        \"system\": \"You are a creative writer.\",\n",
        "        \"user\": \"Write a short poem about AI.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Testing inference with fine-tuned model:\\n\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"System: {prompt['system']}\")\n",
        "    print(f\"User: {prompt['user']}\")\n",
        "\n",
        "    response = generate_response(prompt['system'], prompt['user'])\n",
        "\n",
        "    print(f\"Assistant: {response}\")\n",
        "    print(\"-\" * 60 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "XDSoCtKeqIvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n✓ Script completed successfully!\")\n",
        "print(f\"\\nModel adapter saved to: {ADAPTER_DIR}\")\n",
        "print(f\"Full training logs saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "2wyZp3cgqIsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load and use the fine-tuned adapter in a new session:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Load base model in 4-bit\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-7B\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"./qwen2.5-7b-qlora-adapter\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./qwen2.5-7b-qlora-adapter\")\n",
        "\n",
        "# Use for inference\n",
        "model.eval()\n",
        "```"
      ],
      "metadata": {
        "id": "Ib1RZYTgqfPN"
      }
    }
  ]
}